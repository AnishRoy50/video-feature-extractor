{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"skhH9QGrVpLj","outputId":"f574a3d9-d258-4d92-9788-9421546b0261","executionInfo":{"status":"ok","timestamp":1762877672249,"user_tz":-360,"elapsed":7309,"user":{"displayName":"Anish Roy","userId":"02703942347874961244"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n","Collecting easyocr\n","  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n","Collecting ultralytics\n","  Downloading ultralytics-8.3.227-py3-none-any.whl.metadata (37 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.8.0+cu126)\n","Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.23.0+cu126)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (from easyocr) (4.12.0.88)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.16.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from easyocr) (11.3.0)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.25.2)\n","Collecting python-bidi (from easyocr)\n","  Downloading python_bidi-0.6.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from easyocr) (6.0.3)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.1.2)\n","Collecting pyclipper (from easyocr)\n","  Downloading pyclipper-1.3.0.post6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n","Collecting ninja (from easyocr)\n","  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n","Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.10.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.4.0)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (2.37.2)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (2025.10.16)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (0.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->easyocr) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->easyocr) (3.0.3)\n","Downloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics-8.3.227-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n","Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyclipper-1.3.0.post6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (963 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m963.8/963.8 kB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_bidi-0.6.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m300.6/300.6 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: python-bidi, pyclipper, ninja, ultralytics-thop, ultralytics, easyocr\n","Successfully installed easyocr-1.7.2 ninja-1.13.0 pyclipper-1.3.0.post6 python-bidi-0.6.7 ultralytics-8.3.227 ultralytics-thop-2.0.18\n"]}],"source":["# Install all required packages\n","!pip install opencv-python numpy easyocr ultralytics"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"0JVLLwj1nCsl","outputId":"375081c5-c5f9-432f-d7bd-dd1e08aac945","executionInfo":{"status":"ok","timestamp":1762877705282,"user_tz":-360,"elapsed":32994,"user":{"displayName":"Anish Roy","userId":"02703942347874961244"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","Mounted at /content/drive\n"]}],"source":["# Import all required libraries\n","import cv2\n","import numpy as np\n","import easyocr\n","import torch\n","import json\n","import re\n","from collections import Counter\n","from ultralytics import YOLO\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"RWRfeJzdtmeq"},"source":["# **Motion Analysis:** *Quantify the average motion using Optical Flow.*"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"bsD_Ela3rUZF","executionInfo":{"status":"ok","timestamp":1762877705325,"user_tz":-360,"elapsed":35,"user":{"displayName":"Anish Roy","userId":"02703942347874961244"}}},"outputs":[],"source":["def analyze_motion(video_path: str) -> float:\n","    \"\"\"\n","    Quantify average motion in a video using Optical Flow.\n","\n","    Args:\n","        video_path: Path to the video file\n","\n","    Returns:\n","        Average motion magnitude across all frames\n","    \"\"\"\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        raise ValueError(f\"Cannot open video file: {video_path}\")\n","\n","    ret, prev_frame = cap.read()\n","    if not ret:\n","        cap.release()\n","        raise ValueError(\"Cannot read first frame from video\")\n","\n","    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n","    motion_magnitudes = []\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","        # Calculate optical flow\n","        flow = cv2.calcOpticalFlowFarneback(\n","            prev_gray, gray, None,\n","            pyr_scale=0.5, levels=3, winsize=15,\n","            iterations=3, poly_n=5, poly_sigma=1.2, flags=0\n","        )\n","\n","        magnitude, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n","        motion_magnitudes.append(np.mean(magnitude))\n","        prev_gray = gray\n","\n","    cap.release()\n","    return float(np.mean(motion_magnitudes)) if motion_magnitudes else 0.0"]},{"cell_type":"markdown","metadata":{"id":"XOchSKNC6J40"},"source":["#**Text Detection (OCR):** *Determine text_present_ratio or extract keywords.*"]},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":true,"id":"evQEvFoeUwzo","executionInfo":{"status":"ok","timestamp":1762877705337,"user_tz":-360,"elapsed":6,"user":{"displayName":"Anish Roy","userId":"02703942347874961244"}}},"outputs":[],"source":["def clean_words(text: str) -> list:\n","    \"\"\"Extract and clean words from text (alphabetic, lowercase, length >= 3).\"\"\"\n","    words = re.findall(r\"[A-Za-z]+\", text.lower())\n","    return [w for w in words if len(w) >= 3]\n","\n","\n","def analyze_text_ocr_easyocr(\n","    video_path: str,\n","    ocr_frame_step: int = 10,\n","    languages: list = None,\n","    min_conf: float = 0.5,\n",") -> dict:\n","    \"\"\"\n","    Analyze text presence in video using EasyOCR.\n","\n","    Args:\n","        video_path: Path to video file\n","        ocr_frame_step: Process every Nth frame (default: 10)\n","        languages: List of language codes\n","        min_conf: Minimum confidence threshold (default: 0.5)\n","\n","    Returns:\n","        Dictionary with text_present_ratio, keywords, and file path\n","    \"\"\"\n","    languages = languages or ['en']\n","    use_gpu = torch.cuda.is_available()\n","    print(f\"Using GPU: {use_gpu}\")\n","\n","    reader = easyocr.Reader(languages, gpu=use_gpu)\n","    cap = cv2.VideoCapture(video_path)\n","\n","    if not cap.isOpened():\n","        raise ValueError(f\"Cannot open video file: {video_path}\")\n","\n","    ocr_total_frames = 0\n","    ocr_text_frames = 0\n","    all_words = []\n","    frame_idx = 0\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        if frame_idx % ocr_frame_step == 0:\n","            ocr_total_frames += 1\n","            results = reader.readtext(frame, detail=True)\n","\n","            valid_texts = [\n","                text for (bbox, text, conf) in results\n","                if conf >= min_conf and text.strip()\n","            ]\n","\n","            if valid_texts:\n","                ocr_text_frames += 1\n","                for text in valid_texts:\n","                    all_words.extend(clean_words(text))\n","\n","        frame_idx += 1\n","\n","    cap.release()\n","\n","    text_present_ratio = (\n","        ocr_text_frames / ocr_total_frames if ocr_total_frames > 0 else 0.0\n","    )\n","    keywords = list(Counter(all_words).keys())\n","\n","    return {\n","        \"file\": video_path,\n","        \"text_present_ratio\": float(text_present_ratio),\n","        \"keywords\": keywords,\n","    }"]},{"cell_type":"markdown","metadata":{"id":"ELkLMZvTer8R"},"source":["# **Shot Cut Detection:** *Calculate the number of \"hard cuts.\"*"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"kZls2K6Fe_FM","executionInfo":{"status":"ok","timestamp":1762877705363,"user_tz":-360,"elapsed":21,"user":{"displayName":"Anish Roy","userId":"02703942347874961244"}}},"outputs":[],"source":["def detect_hard_cuts(\n","    video_path: str,\n","    diff_threshold: float = 30.0,\n","    min_scene_length: int = 5\n",") -> dict:\n","    \"\"\"\n","    Detect hard cuts (abrupt shot changes) in a video.\n","\n","    Args:\n","        video_path: Path to video file\n","        diff_threshold: Threshold on mean abs diff (0-255) to detect a cut\n","        min_scene_length: Minimum frames between cuts to avoid flicker detection\n","\n","    Returns:\n","        Dictionary with hard_cuts count, cut_frames list, and file path\n","    \"\"\"\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        raise ValueError(f\"Cannot open video file: {video_path}\")\n","\n","    ret, prev_frame = cap.read()\n","    if not ret:\n","        cap.release()\n","        raise ValueError(\"Cannot read first frame from video\")\n","\n","    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n","    cut_frames = []\n","    frame_idx = 1\n","    last_cut_frame = -min_scene_length\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","        diff_score = float(np.mean(cv2.absdiff(gray, prev_gray)))\n","\n","        # Detect hard cut\n","        if diff_score > diff_threshold and (frame_idx - last_cut_frame) >= min_scene_length:\n","            cut_frames.append(frame_idx)\n","            last_cut_frame = frame_idx\n","\n","        prev_gray = gray\n","        frame_idx += 1\n","\n","    cap.release()\n","\n","    return {\n","        \"file\": video_path,\n","        \"hard_cuts\": len(cut_frames),\n","        \"cut_frames\": cut_frames,\n","    }"]},{"cell_type":"markdown","metadata":{"id":"94RkzByFjS1t"},"source":["# **Object vs. Person Dominance:** *Calculate a ratio of people versus objects detected using a pre-trained model YOLO.*"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"cTxmZye7kFVl","executionInfo":{"status":"ok","timestamp":1762877705454,"user_tz":-360,"elapsed":88,"user":{"displayName":"Anish Roy","userId":"02703942347874961244"}}},"outputs":[],"source":["def analyze_person_object_dominance(\n","    video_path: str,\n","    model_path: str = \"yolov8x.pt\",\n","    det_frame_step: int = 10,\n","    conf_thres: float = 0.5,\n",") -> dict:\n","    \"\"\"\n","    Analyze person vs object dominance in video using YOLO.\n","\n","    Args:\n","        video_path: Path to video file\n","        model_path: YOLO model weights (e.g., 'yolov8x.pt')\n","        det_frame_step: Run detection on every Nth frame\n","        conf_thres: Minimum confidence for counting a detection\n","\n","    Returns:\n","        Dictionary with detection counts and ratios\n","    \"\"\"\n","    model = YOLO(model_path)\n","    cap = cv2.VideoCapture(video_path)\n","\n","    if not cap.isOpened():\n","        raise ValueError(f\"Cannot open video file: {video_path}\")\n","\n","    person_detections = 0\n","    object_detections = 0\n","    frame_idx = 0\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        # Run YOLO on sampled frames\n","        if frame_idx % det_frame_step == 0:\n","            results = model(frame, verbose=False)\n","\n","            if results:\n","                boxes = results[0].boxes\n","                if boxes is not None:\n","                    for box in boxes:\n","                        if float(box.conf[0]) >= conf_thres:\n","                            class_name = model.names[int(box.cls[0])]\n","                            if class_name == \"person\":\n","                                person_detections += 1\n","                            else:\n","                                object_detections += 1\n","\n","        frame_idx += 1\n","\n","    cap.release()\n","\n","    # Calculate ratios\n","    total_detections = person_detections + object_detections\n","    person_ratio = person_detections / total_detections if total_detections > 0 else 0.0\n","    object_ratio = object_detections / total_detections if total_detections > 0 else 0.0\n","    person_to_object_ratio = person_detections / object_detections if object_detections > 0 else None\n","\n","    return {\n","        \"file\": video_path,\n","        \"person_detections\": person_detections,\n","        \"object_detections\": object_detections,\n","        \"person_ratio\": float(person_ratio),\n","        \"object_ratio\": float(object_ratio),\n","        \"person_to_object_ratio\": person_to_object_ratio,\n","    }"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"iQMFYqjys6ND","executionInfo":{"status":"ok","timestamp":1762877705462,"user_tz":-360,"elapsed":5,"user":{"displayName":"Anish Roy","userId":"02703942347874961244"}}},"outputs":[],"source":["def extract_video_features(\n","    video_path: str,\n","    ocr_frame_step: int = 10,\n","    det_frame_step: int = 10,\n","    yolo_model: str = \"yolov8x.pt\"\n",") -> dict:\n","    \"\"\"\n","    Extract comprehensive features from a video file.\n","\n","    This function combines all feature extraction methods:\n","    - Hard cuts detection\n","    - Motion analysis (optical flow)\n","    - Text detection (OCR)\n","    - Person vs object dominance (YOLO)\n","\n","    Args:\n","        video_path: Path to the video file\n","        ocr_frame_step: Sample every Nth frame for OCR (default: 10)\n","        det_frame_step: Sample every Nth frame for YOLO detection (default: 10)\n","        yolo_model: YOLO model to use (default: \"yolov8n.pt\")\n","\n","    Returns:\n","        Dictionary containing all extracted features\n","    \"\"\"\n","    print(f\"Processing video: {video_path}\\n\")\n","\n","    # Extract features from different analyses\n","    print(\"1. Detecting hard cuts...\")\n","    cut_info = detect_hard_cuts(video_path)\n","\n","    print(\"2. Analyzing motion...\")\n","    avg_motion = analyze_motion(video_path)\n","\n","    print(\"3. Detecting text with OCR...\")\n","    ocr_info = analyze_text_ocr_easyocr(\n","        video_path,\n","        ocr_frame_step=ocr_frame_step,\n","        languages=['en'],\n","        min_conf=0.5,\n","    )\n","\n","    print(\"4. Analyzing person vs object dominance...\")\n","    dominance_info = analyze_person_object_dominance(\n","        video_path,\n","        model_path=yolo_model,\n","        det_frame_step=det_frame_step,\n","        conf_thres=0.5,\n","    )\n","\n","    # Combine all features into a single dictionary\n","    features = {\n","        \"file\": video_path,\n","        \"hard_cuts\": cut_info[\"hard_cuts\"],\n","        \"cut_frames\": cut_info[\"cut_frames\"],\n","        \"average_motion\": float(avg_motion),\n","        \"text_present_ratio\": ocr_info[\"text_present_ratio\"],\n","        \"keywords\": ocr_info[\"keywords\"],\n","        \"person_detections\": dominance_info[\"person_detections\"],\n","        \"object_detections\": dominance_info[\"object_detections\"],\n","        \"person_ratio\": dominance_info[\"person_ratio\"],\n","        \"object_ratio\": dominance_info[\"object_ratio\"],\n","        \"person_to_object_ratio\": dominance_info[\"person_to_object_ratio\"],\n","    }\n","\n","    print(\"\\n‚úÖ Feature extraction complete!\")\n","    return features"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"5CzgCH4VtDeP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762877848858,"user_tz":-360,"elapsed":143389,"user":{"displayName":"Anish Roy","userId":"02703942347874961244"}},"outputId":"70197237-f196-4f7d-d8f0-920997f25adc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing video: /content/drive/MyDrive/Test_Video/new.mp4\n","\n","1. Detecting hard cuts...\n","2. Analyzing motion...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"]},{"output_type":"stream","name":"stdout","text":["3. Detecting text with OCR...\n","üîß Using GPU: True\n","Progress: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100.0% Complete"]},{"output_type":"stream","name":"stderr","text":["WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"]},{"output_type":"stream","name":"stdout","text":["Progress: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100.0% Complete4. Analyzing person vs object dominance...\n","\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6.2MB 115.3MB/s 0.1s\n","\n","‚úÖ Feature extraction complete!\n","\n","============================================================\n","EXTRACTED FEATURES\n","============================================================\n","{\n","  \"file\": \"/content/drive/MyDrive/Test_Video/new.mp4\",\n","  \"hard_cuts\": 3,\n","  \"cut_frames\": [\n","    832,\n","    1150,\n","    1348\n","  ],\n","  \"average_motion\": 0.23675572872161865,\n","  \"text_present_ratio\": 0.9863945578231292,\n","  \"keywords\": [\n","    \"english\",\n","    \"thank\",\n","    \"you\",\n","    \"father\",\n","    \"used\",\n","    \"always\",\n","    \"tell\",\n","    \"something\",\n","    \"speeches\",\n","    \"alwaya\",\n","    \"which\",\n","    \"want\",\n","    \"share\",\n","    \"with\",\n","    \"that\",\n","    \"fit\",\n","    \"inside\",\n","    \"glass\",\n","    \"slipper\",\n","    \"why\",\n","    \"know\",\n","    \"like\",\n","    \"were\",\n","    \"told\",\n","    \"cinderella\",\n","    \"did\",\n","    \"dosyde\",\n","    \"dosyoe\",\n","    \"when\",\n","    \"can\",\n","    \"shatter\",\n","    \"the\",\n","    \"ceiling\",\n","    \"iwant\",\n","    \"little\",\n","    \"secret\",\n","    \"not\",\n","    \"very\",\n","    \"iond\",\n","    \"this\",\n","    \"phrase\",\n","    \"breaking\",\n","    \"fond\",\n","    \"speechies\",\n","    \"does\",\n","    \"annoy\",\n","    \"enclish\",\n","    \"because\",\n","    \"takes\",\n","    \"context\",\n","    \"everything\",\n","    \"have\",\n","    \"done\",\n","    \"all\",\n","    \"myachievements\",\n","    \"hard\",\n","    \"work\",\n","    \"and\",\n","    \"puts\",\n","    \"into\",\n","    \"box\",\n","    \"achievements\",\n","    \"spleches\",\n","    \"ambition\",\n","    \"was\",\n","    \"find\",\n","    \"break\",\n","    \"peng\",\n","    \"yeng\",\n","    \"iwas\",\n","    \"never\",\n","    \"mission\",\n","    \"really\",\n","    \"honest\",\n","    \"dengv\",\n","    \"thu\",\n","    \"dengy\",\n","    \"deng\",\n","    \"denc\",\n","    \"den\"\n","  ],\n","  \"person_detections\": 111,\n","  \"object_detections\": 1,\n","  \"person_ratio\": 0.9910714285714286,\n","  \"object_ratio\": 0.008928571428571428,\n","  \"person_to_object_ratio\": 111.0\n","}\n","\n","üíæ Features saved to: /content/drive/MyDrive/Test_Video/video_features.json\n"]}],"source":["# Example usage\n","if __name__ == \"__main__\":\n","    # Update this path to your video file\n","    video_path = \"/content/drive/MyDrive/Test_Video/new.mp4\"\n","\n","    # Extract all features\n","    features = extract_video_features(video_path)\n","\n","    # Display results\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"EXTRACTED FEATURES\")\n","    print(\"=\"*60)\n","    print(json.dumps(features, indent=2))\n","\n","    # Save to JSON file\n","    output_path = \"/content/drive/MyDrive/Test_Video/video_features.json\"\n","    with open(output_path, \"w\") as f:\n","        json.dump(features, f, indent=2)\n","\n","    print(f\"\\nüíæ Features saved to: {output_path}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":".venv (3.14.0)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.14.0"}},"nbformat":4,"nbformat_minor":0}