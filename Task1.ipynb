{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNv/5/VHAxx2rQ5JRTtsbFl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"skhH9QGrVpLj","executionInfo":{"status":"ok","timestamp":1762861500867,"user_tz":-360,"elapsed":5305,"user":{"displayName":"Anish Roy","userId":"02703942347874961244"}},"outputId":"420f5c24-d6d3-4358-c265-f9de2d76b9dd","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n"]}],"source":["!pip install opencv-python numpy\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0JVLLwj1nCsl","executionInfo":{"status":"ok","timestamp":1762861504712,"user_tz":-360,"elapsed":3831,"user":{"displayName":"Anish Roy","userId":"02703942347874961244"}},"outputId":"d9c34f83-0209-4533-b065-9508203d03b0","collapsed":true},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["# **Motion Analysis:** *Quantify the average motion using Optical Flow.*"],"metadata":{"id":"RWRfeJzdtmeq"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","\n","def analyze_motion(video_path: str) -> float:\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        print(\"Error: Cannot open video file.\")\n","        return 0.0\n","\n","    ret, prev_frame = cap.read()\n","    if not ret:\n","        print(\"Error: Cannot read first frame.\")\n","        return 0.0\n","\n","    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n","    motion_magnitudes = []\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","        # Calculate optical flow between prev_gray and gray\n","        flow = cv2.calcOpticalFlowFarneback(\n","            prev_gray, gray, None,\n","            pyr_scale=0.5, levels=3, winsize=15,\n","            iterations=3, poly_n=5, poly_sigma=1.2, flags=0\n","        )\n","\n","        magnitude, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n","        motion_magnitudes.append(np.mean(magnitude))\n","\n","        prev_gray = gray\n","\n","    cap.release()\n","\n","    avg_motion = float(np.mean(motion_magnitudes)) if motion_magnitudes else 0.0\n","    return avg_motion\n","\n","\n"],"metadata":{"id":"bsD_Ela3rUZF","executionInfo":{"status":"ok","timestamp":1762861504753,"user_tz":-360,"elapsed":13,"user":{"displayName":"Anish Roy","userId":"02703942347874961244"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["#**Text Detection (OCR):** *Determine a text_present_ratio or extract keywords.*"],"metadata":{"id":"XOchSKNC6J40"}},{"cell_type":"code","source":["!pip install easyocr\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"9Mol9ZNhUiW3","executionInfo":{"status":"ok","timestamp":1762861516409,"user_tz":-360,"elapsed":11646,"user":{"displayName":"Anish Roy","userId":"02703942347874961244"}},"outputId":"43452f44-f339-4f7b-bd66-69ebf18d181b"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: easyocr in /usr/local/lib/python3.12/dist-packages (1.7.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.8.0+cu126)\n","Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.23.0+cu126)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (from easyocr) (4.12.0.88)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.16.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.0.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from easyocr) (11.3.0)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.25.2)\n","Requirement already satisfied: python-bidi in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.6.7)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from easyocr) (6.0.3)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.1.2)\n","Requirement already satisfied: pyclipper in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.3.0.post6)\n","Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.13.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.4.0)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (2.37.2)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (2025.10.16)\n","Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (25.0)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (0.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->easyocr) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->easyocr) (3.0.3)\n"]}]},{"cell_type":"code","source":["import cv2\n","import easyocr\n","import re\n","from collections import Counter\n","import torch  # to check GPU availability\n","\n","\n","def clean_words(text: str):\n","    \"\"\"\n","    Basic cleanup:\n","      - keep only alphabetic tokens\n","      - lowercase\n","      - length >= 3\n","    \"\"\"\n","    words = re.findall(r\"[A-Za-z]+\", text.lower())\n","    filtered = [w for w in words if len(w) >= 3]\n","    return filtered\n","\n","\n","def analyze_text_ocr_easyocr(\n","    video_path: str,\n","    ocr_frame_step: int = 10,\n","    languages=None,\n","    min_conf: float = 0.5,\n",") -> dict:\n","    \"\"\"\n","    Analyze a video using EasyOCR:\n","      - text_present_ratio: fraction of sampled frames that contain text\n","      - keywords: cleaned list of unique words detected by OCR\n","      - Automatically uses GPU if available\n","    \"\"\"\n","    if languages is None:\n","        languages = ['en']\n","\n","    # ðŸ” Check for GPU\n","    use_gpu = torch.cuda.is_available()\n","    print(f\"ðŸ”§ Using GPU: {use_gpu}\")\n","\n","    # Initialize EasyOCR reader\n","    reader = easyocr.Reader(languages, gpu=use_gpu)\n","\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        print(\"Error: Cannot open video file.\")\n","        return {\n","            \"file\": video_path,\n","            \"text_present_ratio\": 0.0,\n","            \"keywords\": [],\n","        }\n","\n","    ocr_total_frames = 0\n","    ocr_text_frames = 0\n","    all_words = []\n","\n","    frame_idx = 0\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        # Sample every Nth frame for OCR\n","        if frame_idx % ocr_frame_step == 0:\n","            ocr_total_frames += 1\n","\n","            # results: list of [bbox, text, confidence]\n","            results = reader.readtext(frame, detail=True)\n","\n","            # Keep only high-confidence texts\n","            valid_texts = [\n","                text for (bbox, text, conf) in results\n","                if conf >= min_conf and text.strip()\n","            ]\n","\n","            if valid_texts:\n","                ocr_text_frames += 1\n","                for t in valid_texts:\n","                    all_words.extend(clean_words(t))\n","\n","        frame_idx += 1\n","\n","    cap.release()\n","\n","    text_present_ratio = (\n","        ocr_text_frames / ocr_total_frames if ocr_total_frames > 0 else 0.0\n","    )\n","\n","    keyword_counter = Counter(all_words)\n","    keywords = list(keyword_counter.keys())\n","\n","    return {\n","        \"file\": video_path,\n","        \"text_present_ratio\": float(text_present_ratio),\n","        \"keywords\": keywords,\n","    }\n","\n","\n","# if __name__ == \"__main__\":\n","#     video_path = \"/content/drive/MyDrive/Test_Video/test_video_subtitle.mp4\"\n","#     features = analyze_text_ocr_easyocr(video_path)\n","#     print(features)\n"],"metadata":{"collapsed":true,"id":"evQEvFoeUwzo","executionInfo":{"status":"ok","timestamp":1762861516432,"user_tz":-360,"elapsed":19,"user":{"displayName":"Anish Roy","userId":"02703942347874961244"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["# **Shot Cut Detection:** *Calculate the number of \"hard cuts.\"*"],"metadata":{"id":"ELkLMZvTer8R"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","\n","\n","def detect_hard_cuts(\n","    video_path: str,\n","    diff_threshold: float = 30.0,\n","    min_scene_length: int = 5\n",") -> dict:\n","    \"\"\"\n","    Detect hard cuts (abrupt shot changes) in a video.\n","\n","    A \"hard cut\" is detected when the mean pixel difference between\n","    consecutive frames exceeds `diff_threshold`.\n","\n","    :param video_path: path to the video file\n","    :param diff_threshold: threshold on mean abs diff to decide a cut (0â€“255)\n","    :param min_scene_length: minimum number of frames between cuts\n","                             (to avoid counting tiny flickers as multiple cuts)\n","    :return: dict with number of cuts and frame indices of cuts\n","    \"\"\"\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        print(\"Error: Cannot open video file.\")\n","        return {\n","            \"file\": video_path,\n","            \"hard_cuts\": 0,\n","            \"cut_frames\": [],\n","        }\n","\n","    # Read first frame\n","    ret, prev_frame = cap.read()\n","    if not ret:\n","        print(\"Error: Cannot read first frame.\")\n","        cap.release()\n","        return {\n","            \"file\": video_path,\n","            \"hard_cuts\": 0,\n","            \"cut_frames\": [],\n","        }\n","\n","    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n","\n","    hard_cuts = 0\n","    cut_frames = []\n","    frame_idx = 1          # we already read frame 0\n","    last_cut_frame = -min_scene_length  # so first cut is not blocked\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","        # Mean absolute difference between consecutive frames\n","        diff = cv2.absdiff(gray, prev_gray)\n","        diff_score = float(np.mean(diff))  # 0â€“255\n","\n","        # Decide if this is a hard cut\n","        if diff_score > diff_threshold and (frame_idx - last_cut_frame) >= min_scene_length:\n","            hard_cuts += 1\n","            cut_frames.append(frame_idx)\n","            last_cut_frame = frame_idx\n","\n","        prev_gray = gray\n","        frame_idx += 1\n","\n","    cap.release()\n","\n","    return {\n","        \"file\": video_path,\n","        \"hard_cuts\": hard_cuts,\n","        \"cut_frames\": cut_frames,\n","    }\n","\n","\n","# if __name__ == \"__main__\":\n","#     video_path = \"/content/drive/MyDrive/Test_Video/hard_cut.mp4\"\n","#     result = detect_hard_cuts(video_path)\n","#     print(result)\n"],"metadata":{"id":"kZls2K6Fe_FM","executionInfo":{"status":"ok","timestamp":1762861516437,"user_tz":-360,"elapsed":1,"user":{"displayName":"Anish Roy","userId":"02703942347874961244"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["# **Object vs. Person Dominance:** *Calculate a ratio of people versus objects detected using a pre-trained model YOLO.*"],"metadata":{"id":"94RkzByFjS1t"}},{"cell_type":"code","source":["!pip install ultralytics\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"LUrpxu26i5Ld","executionInfo":{"status":"ok","timestamp":1762861526849,"user_tz":-360,"elapsed":10402,"user":{"displayName":"Anish Roy","userId":"02703942347874961244"}},"outputId":"89fd7c6c-2857-4b3e-8a2e-0af37fa76df4"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.227)\n","Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n","Requirement already satisfied: ultralytics-thop>=2.0.18 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.18)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.10.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n"]}]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from ultralytics import YOLO\n","\n","\n","def analyze_person_object_dominance(\n","    video_path: str,\n","    model_path: str = \"yolov8x.pt\",\n","    det_frame_step: int = 10,\n","    conf_thres: float = 0.5,\n",") -> dict:\n","    \"\"\"\n","    Analyze a video using YOLO to estimate person vs object dominance.\n","\n","    - person_detections: total person boxes across sampled frames\n","    - object_detections: total non-person boxes\n","    - person_ratio: person_detections / (person + object)\n","    - object_ratio: object_detections / (person + object)\n","    - person_to_object_ratio: person_detections / object_detections\n","\n","    :param video_path: path to the video file\n","    :param model_path: YOLO model weights (e.g. 'yolov8n.pt')\n","    :param det_frame_step: run detection on every Nth frame\n","    :param conf_thres: minimum confidence for counting a detection\n","    :return: dict with counts and ratios\n","    \"\"\"\n","\n","    # Load YOLO model (pretrained on COCO by default)\n","    model = YOLO(model_path)\n","\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        print(\"Error: Cannot open video file.\")\n","        return {\n","            \"file\": video_path,\n","            \"person_detections\": 0,\n","            \"object_detections\": 0,\n","            \"person_ratio\": 0.0,\n","            \"object_ratio\": 0.0,\n","            \"person_to_object_ratio\": None,\n","        }\n","\n","    person_detections = 0\n","    object_detections = 0\n","    frame_idx = 0\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        # Only run YOLO on every Nth frame for speed\n","        if frame_idx % det_frame_step == 0:\n","            # Run YOLO; results is a list of Results objects\n","            results = model(frame, verbose=False)\n","\n","            if not results:\n","                frame_idx += 1\n","                continue\n","\n","            result = results[0]  # single image\n","\n","            boxes = result.boxes\n","            if boxes is not None and len(boxes) > 0:\n","                for box in boxes:\n","                    cls_id = int(box.cls[0])\n","                    conf = float(box.conf[0])\n","\n","                    if conf < conf_thres:\n","                        continue\n","\n","                    class_name = model.names[cls_id]\n","                    if class_name == \"person\":\n","                        person_detections += 1\n","                    else:\n","                        object_detections += 1\n","\n","        frame_idx += 1\n","\n","    cap.release()\n","\n","    total_detections = person_detections + object_detections\n","    if total_detections > 0:\n","        person_ratio = person_detections / total_detections\n","        object_ratio = object_detections / total_detections\n","    else:\n","        person_ratio = 0.0\n","        object_ratio = 0.0\n","\n","    if object_detections > 0:\n","        person_to_object_ratio = person_detections / object_detections\n","    else:\n","        # If there are no objects but people exist, dominance is effectively \"infinite\"\n","        person_to_object_ratio = None  # or float(\"inf\") if you prefer\n","\n","    return {\n","        \"file\": video_path,\n","        \"person_detections\": int(person_detections),\n","        \"object_detections\": int(object_detections),\n","        \"person_ratio\": float(person_ratio),\n","        \"object_ratio\": float(object_ratio),\n","        \"person_to_object_ratio\": person_to_object_ratio,\n","    }\n"],"metadata":{"id":"cTxmZye7kFVl","executionInfo":{"status":"ok","timestamp":1762861526883,"user_tz":-360,"elapsed":25,"user":{"displayName":"Anish Roy","userId":"02703942347874961244"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["# if __name__ == \"__main__\":\n","#     video_path = \"/content/drive/MyDrive/Test_Video/hard_cut_test.mp4\"\n","#     features = analyze_person_object_dominance(video_path)\n","#     print(features)"],"metadata":{"id":"w-TpG6M2rdj4","executionInfo":{"status":"ok","timestamp":1762861526894,"user_tz":-360,"elapsed":2,"user":{"displayName":"Anish Roy","userId":"02703942347874961244"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","def extract_video_features(video_path: str) -> dict:\n","    \"\"\"\n","    Run all feature extractors on a video and return a unified feature dict.\n","\n","    Features:\n","      - hard_cuts, cut_frames\n","      - average_motion\n","      - text_present_ratio, keywords\n","      - person/object dominance metrics\n","    \"\"\"\n","    # ---- 1. Shot cut detection ----\n","    cut_info = detect_hard_cuts(video_path)\n","    # cut_info -> { \"file\": ..., \"hard_cuts\": int, \"cut_frames\": [...] }\n","\n","    # ---- 2. Motion analysis ----\n","    avg_motion = analyze_motion(video_path)\n","    # avg_motion -> float\n","\n","    # ---- 3. Text detection (OCR via EasyOCR) ----\n","    ocr_info = analyze_text_ocr_easyocr(\n","        video_path,\n","        ocr_frame_step=10,   # sample every 10th frame for speed\n","        languages=['en'],    # adjust if you have other languages\n","        min_conf=0.5,\n","    )\n","    # ocr_info -> { \"file\": ..., \"text_present_ratio\": float, \"keywords\": [...] }\n","\n","    # ---- 4. Person vs object dominance (YOLO) ----\n","    dominance_info = analyze_person_object_dominance(\n","        video_path,\n","        model_path=\"yolov8n.pt\",  # or yolov8s.pt if you want more accuracy\n","        det_frame_step=10,\n","        conf_thres=0.5,\n","    )\n","    # dominance_info -> { \"file\": ..., \"person_detections\": ..., \"object_detections\": ..., ... }\n","\n","    # ---- 5. Combine everything in ONE structured dict ----\n","    features = {\n","        \"file\": video_path,\n","\n","        # Shot cuts\n","        \"hard_cuts\": cut_info.get(\"hard_cuts\", 0),\n","        \"cut_frames\": cut_info.get(\"cut_frames\", []),\n","\n","        # Motion\n","        \"average_motion\": float(avg_motion),\n","\n","        # OCR / text presence\n","        \"text_present_ratio\": float(ocr_info.get(\"text_present_ratio\", 0.0)),\n","        \"keywords\": ocr_info.get(\"keywords\", []),\n","\n","        # Person vs object dominance\n","        \"person_detections\": int(dominance_info.get(\"person_detections\", 0)),\n","        \"object_detections\": int(dominance_info.get(\"object_detections\", 0)),\n","        \"avg_persons_per_sampled_frame\": float(\n","            dominance_info.get(\"avg_persons_per_sampled_frame\", 0.0)\n","        ),\n","        \"person_frame_ratio\": float(\n","            dominance_info.get(\"person_frame_ratio\", 0.0)\n","        ),\n","        \"person_ratio\": float(dominance_info.get(\"person_ratio\", 0.0)),\n","        \"object_ratio\": float(dominance_info.get(\"object_ratio\", 0.0)),\n","    }\n","\n","    return features\n"],"metadata":{"id":"iQMFYqjys6ND","executionInfo":{"status":"ok","timestamp":1762861526914,"user_tz":-360,"elapsed":14,"user":{"displayName":"Anish Roy","userId":"02703942347874961244"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    video_path = \"/content/drive/MyDrive/Test_Video/hard_cut_test.mp4\"\n","\n","    features = extract_video_features(video_path)\n","    print(\"Extracted features:\\n\", features)\n","\n","    # Save to JSON file\n","    output_path = \"/content/drive/MyDrive/Test_Video/video_features.json\"\n","    with open(output_path, \"w\") as f:\n","        json.dump(features, f, indent=2)\n","\n","    print(f\"\\nFeatures saved to: {output_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5CzgCH4VtDeP","executionInfo":{"status":"ok","timestamp":1762862056211,"user_tz":-360,"elapsed":529289,"user":{"displayName":"Anish Roy","userId":"02703942347874961244"}},"outputId":"f62f279e-b1b0-49c8-8aff-43e35bb55708"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ”§ Using GPU: True\n","Extracted features:\n"," {'file': '/content/drive/MyDrive/Test_Video/hard_cut_test.mp4', 'hard_cuts': 35, 'cut_frames': [22, 54, 70, 89, 119, 140, 157, 164, 169, 185, 203, 225, 251, 273, 280, 286, 291, 304, 311, 337, 366, 386, 404, 421, 449, 474, 505, 521, 543, 565, 602, 628, 656, 690, 720], 'average_motion': 2.1984872817993164, 'text_present_ratio': 0.3424657534246575, 'keywords': ['superi', 'super', 'gund', 'cornchips', 'iguperi', 'cod', 'tasty', 'cord', 'sweet', 'favor', 'crunch', 'prifood', 'hips', 'alouritq', 'asty', 'corn', 'flavor', 'unc', 'cru', 'try', 'them', 'pasalubong', 'style', 'crackling', 'cheese', 'not', 'walght', 'lechon', 'kawali', 'special', 'chip', 'charon', 'fiavor', 'kowali'], 'person_detections': 97, 'object_detections': 20, 'avg_persons_per_sampled_frame': 0.0, 'person_frame_ratio': 0.0, 'person_ratio': 0.8290598290598291, 'object_ratio': 0.17094017094017094}\n","\n","Features saved to: /content/drive/MyDrive/Test_Video/video_features.json\n"]}]}]}